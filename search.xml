<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Python中的赋值、浅拷贝和深拷贝]]></title>
    <url>%2F2019%2F05%2F21%2F11_Python%E4%B8%AD%E7%9A%84%E6%8B%B7%E8%B4%9D%2F</url>
    <content type="text"><![CDATA[今天看了很多爬虫面试题，太尴尬了，几乎都不会，红红火火恍恍惚惚哈哈哈哈，所以我赶紧来查漏补缺了——学习Python中的拷贝啦！每天默念三遍：我爱学习！我爱学习！我爱学习！ Python中的赋值只是引用了变量，不会占用新的内存 浅拷贝有三种形式： 完全切片 list()函数 copy模块中的copy()函数 深拷贝：copy模块中的deepcopy()函数 import copya = [1,2,3,4,[‘x’,’y’]]b = a # 赋值# 浅拷贝c = a[:] # 1、切片d = list(a) # 2、工厂函数e = copy.copy(a) # 3、copy函数# 深拷贝f = copy.deepcopy(a)print(id(a),id(b),id(c),id(d),id(f),id(e))>>> 69613192 69613192 69613704 69614728 69613768 69614792a.append(5)print(a) # a添加了一个元素>>> [1, 2, 3, 4, [‘x’, ‘y’], 5]print(b) # b跟着添加了一个元素>>> [1, 2, 3, 4, [‘x’, ‘y’], 5]print(c) # c、d、e保持不变>>> [1, 2, 3, 4, [‘x’, ‘y’]]print(d)>>> [1, 2, 3, 4, [‘x’, ‘y’]]print(e)>>> [1, 2, 3, 4, [‘x’, ‘y’]]print(f) # f保持不变>>> [1, 2, 3, 4, [‘x’, ‘y’]] 我们在这里定义了一个列表a，a里面第五个元素嵌套了一个列表，使用a.append()函数给a添加了一个元素。被赋值的b跟着添加了一个元素，并打印了a、b的id，说明被赋值的b只是对a的引用，被浅拷贝的c、d、e和被深拷贝的f都没有发生变化。 print(‘*‘*30)a[4].append(‘z’)print(a) # a索引为4所在的列表内部添加了一个元素z>>> [1, 2, 3, 4, [‘x’, ‘y’, ‘z’], 5]print(b) # b索引为4的元素内部跟着添加了一个元素z>>> [1, 2, 3, 4, [‘x’, ‘y’, ‘z’], 5]print(c) # c、d、e的内部元素添加了元素z>>> [1, 2, 3, 4, [‘x’, ‘y’, ‘z’]]print(d)>>> [1, 2, 3, 4, [‘x’, ‘y’, ‘z’]]print(e)>>> [1, 2, 3, 4, [‘x’, ‘y’, ‘z’]]print(f) # f纹丝不动，没有添加任何值>>> [1, 2, 3, 4, [‘x’, ‘y’]]print(id(a),id(b),id(c),id(d),id(f),id(e)) 在这里，我们对a嵌套的列表进行了追加元素的操作，被赋值的b和被浅拷贝的c、d、e都发生了变化，而f依然保持不变。 总结： 不管是对变量外层还是内层进行操作，赋值都会跟着变化 只有对变量内层元素进行操作时，浅拷贝才会发生变化 无论原列表如何变化，深拷贝保持不变]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>拷贝</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬取腾讯四川新闻]]></title>
    <url>%2F2019%2F05%2F20%2F10_%E8%85%BE%E8%AE%AF%E6%96%B0%E9%97%BB%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[周六我去参加初中英语教师资格的面试了，然而，我很怂，在结结巴巴答完结构化问题之后，居然告诉考官：我放弃！考官都震惊了！我真的很怂啊，我觉得我的教案写得非常巴适，可是当你站在讲台上的时候真的什么都讲不出来，也不是紧张，感觉就是很尴尬，匆匆说完原因后我就离场了，虽然我一大早就起床了，在大姨妈问候你的情况下，还排了很久的队，成为考场的最后一位考生。可能我真的不适合，或者又是因为我缺乏这种公开站在大家面前的锻炼。我妈说我只会纸上谈兵，我难得一次赞成她的观点。So much for that day！We should stop here and go to study ‘Python Spider’！在堕(受)落(到)了(hr)两(打)周(击)两周后，终于奋发图强，开始了正儿八经的学习！我爱学习！ 目标 为了练习Selenium和Beautiful Soup 为了获取腾讯四川新闻的相关数据 准备工作在谷歌浏览器中打开网址 https://new.qq.com/d/cd 你会发现页面是通过ajax加载的，可以查看网页源代码发现并没有数据，这时可以你可以尝试找接口或者用selenium + chrome的方式来动态爬取页面数据，今天为了练习selenium，所以就用了selenium来抓取。 开始抓取 先导入需要的模块，然后定义了一个腾讯新闻爬虫类，设置了chromedriver的路径 定义了一个get_one_page()方法，向 https://new.qq.com/d/cd 发送请求。这里定义了一个循环，让代码一直执行下拉操作。一直滑，滑到页面底端，你可以看到页面是是这样的，所以我们在循环内部设置了一个判断方法，当出现“返回腾讯网这几个字的时候，我们就让它停掉。 第二个方法parse_one_page()，用Beautiful Soup库解析页面，获取新闻标题链接，因为在爬取过程中报了Index Error，所以我们在这里进行了异常处理 get_second_page()方法，用requests库发起请求，解析页面，获取文章标题，文章内容和发布时间 save_to_mongo()方法，用pymongo连接数据库，将数据存储到数据库Tencent下的集合News内 成果展示 调用API如果你不想用selenium，你可以获取接口数据，shift+f12，看到这个rcd?开头的文件，右边就是一条条新闻的相关数据，查看他的headers，找到请求的url，有个page参数可实现翻页操作。 每天进步一点点，年薪百万！！！]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬取腾讯四川新闻]]></title>
    <url>%2F2019%2F05%2F20%2F10-%E8%85%BE%E8%AE%AF%E6%96%B0%E9%97%BB%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[周六我去参加初中英语教师资格的面试了，然而，我很怂，在结结巴巴答完结构化问题之后，居然告诉考官：我放弃！考官都震惊了！我真的很怂啊，我觉得我的教案写得非常巴适，可是当你站在讲台上的时候真的什么都讲不出来，也不是紧张，感觉就是很尴尬，匆匆说完原因后我就离场了，虽然我一大早就起床了，在大姨妈问候你的情况下，还排了很久的队，成为考场的最后一位考生。可能我真的不适合，或者又是因为我缺乏这种公开站在大家面前的锻炼。我妈说我只会纸上谈兵，我难得一次赞成她的观点。So much for that day！We should stop here and go to study ‘Python Spider’！在堕(受)落(到)了(hr)两(打)周(击)两周后，终于奋发图强，开始了正儿八经的学习！我爱学习！ 目标 为了练习Selenium和Beautiful Soup 为了爬取腾讯四川新闻 准备工作 在谷歌浏览器中打开网址 https://new.qq.com/d/cd 你会发现页面是通过ajax加载的，可以查看网页源代码发现并没有数据，这时可以你可以尝试找接口或者用selenium + chrome的方式来动态爬取页面数据，今天为了练习selenium，所以就用了selenium来爬取。]]></content>
  </entry>
  <entry>
    <title><![CDATA[小舒的最近两天]]></title>
    <url>%2F2019%2F05%2F10%2F05_%E6%94%BE%E5%BC%83%E7%9A%84%E6%97%B6%E5%80%99%2F</url>
    <content type="text"><![CDATA[众生皆苦，我也苦暴躁小舒，在线怼人，连hr都怼了，我怕是不想活了。想了一下这篇博客的标题应该叫什么好，是“小舒与简历斗智斗勇”，还是“小舒与招聘网站殊死搏斗”，最后还是想了这个。啊，我真的好丧啊，从一大早就开始丧了。大多数时候，痛苦和悲哀都来自于家庭，我真的好想离他们远远的啊。我自己也很烦啊，摆脱就不要再给我传递这些令人头疼的消息了吧。我觉得我真的要放弃了，我做了这么久的努力都是白费的，在找实习的时候一个都找不到。我们本来就是普罗大众，谈什么改变命运，改变不了的。我不想再努力了，放弃的时候真的很舒服。一时堕落一时爽，一直堕落一直爽。你知道吗？我昨天玩屎了，想不到吧，哈哈哈，其实我就吃了个芒果，但是吃个芒果就跟他妈玩屎一样，弄得我一身都是。不过任何人都逃不过真香定律，虽然确实很恶心，但是真的好吃呀，我说的是芒果好吃。昨天和老王差点吵架了，他感觉我很幼稚，很无理取闹吧，但是他应该不知道我有多喜欢他吧。我很想他，上课想、看书想、睡觉也想，我想跟他说话，我也想跟他撒娇，明明我就不是这种人，遇到他居然变成这样了，我好讨厌自己啊啊啊啊。我买了四本故事书，本来是要寄给他让他给我讲睡前故事的，他肯定害怕我是女骗子，所以只有买来自己给他讲睡前故事了，这个男人坏的很呐。如果哪天我不愿意跟他撒娇了，那么他也和其他人没什么区别了，不过，现在，娘爱你（づ￣3￣）づ╭❤～]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>心情</tag>
        <tag>学校</tag>
        <tag>日记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[五月伊始]]></title>
    <url>%2F2019%2F05%2F02%2F09_%E4%BA%94%E6%9C%88%E4%BC%8A%E5%A7%8B%2F</url>
    <content type="text"><![CDATA[我以为世界在等着我长大，没想到是时间推着我向前感觉时间飞逝啊！上一次写博客已经是很久前的事了吧！四月已经结束了！五月又匆匆忙忙地来了！我还是没有什么进步。这个月计算机二级的成绩也要出来了，英语教师资格证面试也快了，最重要的是，七月就要去实习了，五月就要报名是集中实习还是分散实习了，所以我，得开始准备面试了。不管是爬虫还是新闻，简历都备起来了。在写简历的时候才发现自己什么也写不上去。可是生活还是要继续啊，我在努力啊。我也想把年薪过万变成现实啊，想去旅游，想带奶奶吃好吃的，想带弟弟玩好玩的。最近，我是被恋爱冲昏了头脑吧，不！我还没恋呢，不过为什么我那么依赖他，居然在他面前变成了一个小女生？？？what fuck？？？我怎么会变成这种人？？？我不能这样啊，我要好好学习啊，不能浪费时间了。今天他说带我回家过年，虽然我知道可能是玩笑话，不过我还是认真的告诉了他我不愿意。我不是觉得自己要和很优秀（高富帅）的人在一起，毕竟我自己也不怎样，好男人又没瞎眼，毕竟物以类聚，人以群分，可是我还是不想结婚，不是和谁的问题，是我不想。我想象不出来自己结婚后是怎样的？我都怀疑自己要单身一辈子了！也遇不到一个合适的，我觉得就应该听妈妈的话，好好地去相亲，毕竟妈妈看上的人家都不是很差。 不不不！谈什么儿女情长，在这个年龄就应该巨富！！！好好努力吧，老老实实写代码，勤勤恳恳找实习！ （づ￣3￣）づ╭❤～]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>学校</tag>
        <tag>日记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网易云单曲评论]]></title>
    <url>%2F2019%2F05%2F02%2F08_%E7%BD%91%E6%98%93%E4%BA%91%E5%8D%95%E6%9B%B2%E8%AF%84%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[眼看着7月份就要去实习了，还不知道做什么呢，什么都还不会呢，打算着找个爬虫的实习吧，但是我这点水平应该不行吧，所以又来写了个网易单曲评论爬虫。感觉好！不能这么丧！振作！快醒醒！起来撸代码啦！看了一下网易云，以陈雪凝的《绿色》为例，F12键找到了这个R_SO_4_1345848098?csrf_token=，并且点开发现了里面就是我们想要的评论数据再查看请求头,这个requet url：https://music.163.com/weapi/v1/resource/comments/R_SO_4_1345848098?csrf_token=看request method是以post方式提交的然后我屁颠儿屁颠儿地点了个翻页，在评论翻页的时候这个request url并没有刷新！！！没有刷新那这个翻页的操作怎么做？是用selenium还是…？看下面的表单数据应该是经过加密的，所以我在网上百度了一下，果然有大佬破解这个算法加密，参考：https://www.zhihu.com/question/36081767这是算法解密的代码，都是大佬们牛逼啊！我就稍微改了一下，并且把数据存储到MongoDB 完整代码地址见：https://github.com/Awake2714/WangyiComment 不过有点问题的是，我发现在爬到400多页的时候就没数据了，虽然代码一直在跑，而且也没有被ban，但是就是没拿到数据，所以这是怎么回事？是不是像豆瓣一样，评论只能拿取500条？有没有小伙伴知道呀？]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用scrapy框架下载煎蛋网图片]]></title>
    <url>%2F2019%2F04%2F18%2F07_jandan%2F</url>
    <content type="text"><![CDATA[每次写完代码后都感觉自己人老珠黄啊，该拿什么拯救我的老脸？？ spider.py这部分很简单，只需要获取图片链接和下一页的链接就可以了 item.py import scrapyclass JiandanItem(scrapy.Item):&ensp;&ensp;&ensp;&ensp;img = scrapy.Field() pipelines.py这一部分比较重要，scrapy提供了ImagesPipeline，我们只需要继承自这个类，重写其中的一些方法，scrapy就会为我们下载图片了。 settings.py在settings.py里开启相关配置 部分图片展示因为煎蛋网反爬措施没那么厉害，所以很快“唰唰唰”的就下载完了一共四千多张图片，慢慢欣赏吧 biu~ (๑´ڡ`๑)]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫设置随机请求头和代理]]></title>
    <url>%2F2019%2F04%2F11%2F03_%E7%88%AC%E8%99%AB%E8%AE%BE%E7%BD%AE%E9%9A%8F%E6%9C%BA%E8%AF%B7%E6%B1%82%E5%A4%B4%E5%92%8C%E4%BB%A3%E7%90%86%2F</url>
    <content type="text"><![CDATA[我在设置爬虫代理和请求头时，真是遇到了很多坑啊，自己一个人自学，又没人交流，自己又是一个没有耐心的人，简直要爆炸了终于，今天晚上好好地静下来慢慢捋了一下代码！终于，似乎从这个坑出来了！先来说说用requests库设置吧，随机请求头我用的是fake-useragent这个包先用httpbin.org/get这个网址测试了一下就这样设置就可以正确输出了，开始我设置的时候居然直接写的 headers=ua.random 我真的要哭死啊！！！一定要记得构造字典！！！ 设置代理，我买的蘑菇代理，感觉还挺划算的，按数量买1000个6块钱，毕竟测试也用不完这么多，永久有效，爬完某宝后我都还剩700多个；按数量感觉不太划算，一天2000个6块钱吧，不过我也用不完，当然如果你需求大的话另当别论，土豪也请随意哈哈哈这里的坑也很多啊，请求了好多次后都不成功，我把ip打印了一下，好像确实有点问题，不过还是不知道问题确切所在，所以又打印了一下长度，果然！！！里面有个换行符和回车！！！我的娘啊！！！我枯了皇天不负有心人，终于搞好了 接下来终于在scrapy框架里面设置这两个中间件了在网上看到好多方法，自己也懵逼，试了一下，这样也行的：request.headers[‘User-Agent’] = self.ua.random不过我真是傻！这样好像是真的不行！不行！会报错的！request.meta[‘headers’] = {‘User-Agent’: ua.random}因为有些请求是不需要代理的，所以查了一下说什么重写make_requests_from_url()方法，设置meta={‘download_timeout’: 10}，但是不知道为什么我的就是不行，所以干脆没有设置了，如果有大佬知道是怎么回事可不可以告诉我一下啊。 另外我是想运行这个代理池的，但是老是报错，实在不知怎么回事啊[泪奔]]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scrapy爬取某宝商品信息]]></title>
    <url>%2F2019%2F04%2F11%2F04_scrapy%E7%88%AC%E5%8F%96%E6%9F%90%E5%AE%9D%E5%95%86%E5%93%81%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[在多次踩坑后，终于写好我的爬虫了。说说我遇到的问题:第一就是在scrapy框架里设置随机请求头中间件第二也是中间件，代理中间件，关于这两点可以参考我的博客：爬虫设置随机请求头和代理这两个问题真的困扰了我超级久啊。解决好这两个问题好，我的爬虫终于运行起来啦啦啦贴一下我的部分代码，嘿嘿嘿 目录文件夹结构 taobao.py重写 start_requests() 方法parse()方法 解析商品信息 items.py你要存储的字段 middlewares.py随机请求头 UserAgentMiddleware代理 ProxyMiddleware settings.py pipelines.py存储数据到Mongodb run.py最后就可以运行啦 python run.py运行结束后的部分数据是这样的哈哈哈 代码地址：https://github.com/Awake2714/Taobao]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[四分之一]]></title>
    <url>%2F2019%2F03%2F30%2F06_%E5%9B%9B%E5%88%86%E4%B9%8B%E4%B8%80%2F</url>
    <content type="text"><![CDATA[今天是3月的倒数第二天，也是2019四分之一快要结束的倒数第二天。时间过得可真快呀，忙碌的3月终于结束了。以为自己多努力，其实就考了教师资格证和计算机二级，结果怎样也不知道。三月，我习惯了戴上耳机，坐在电脑前的日子；三月，我的生物钟不允许我睡懒觉；三月，把我从熟人社交和陌生人社交中解救出来。大概成长就是逐渐失去联系，大概成长就是在自己以前吹过牛逼的地方填坑癞蛤蟆永远变不成天鹅的，不管你多拼，有些东西天生就注定了。四月，看似清闲，但却有好多事没有做，从何下手，毫无头绪。所以，我们加油吧，希望大家都不要忘记年初立下的flag[牛逼];希望大家2019好好学习[填坑]，好好生活，硕果累累！]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>学校</tag>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[快让我冷静一下]]></title>
    <url>%2F2019%2F03%2F17%2F01_%E5%BF%AB%E8%AE%A9%E6%88%91%E5%86%B7%E9%9D%99%E4%B8%80%E4%B8%8B%2F</url>
    <content type="text"><![CDATA[呜呜呜，太激动了，我快要抑制不住自己的兴奋了在经过so many bugs后我的博客终于问世了本来第一次已经搭建好了，可是在做主题美化时不知道哪里出了问题，导致页面无法正常显示了，对于我一个菜鸟而言，感觉还是直接重来比较快捷，索性把GitHub上的仓库给删了，重新来过。第二次搭建时自然比第一次轻松了许多，在网上找了很多大佬的博客，一步一步地来，虽然中间的坑很多，但终于大致完成了美化工作最后买了个域名，将GitHub域名关联起来，由于昨天没有认证，搞了半天都不知道问题在哪儿。今天晚上又来搞了一下，啊啊啊，终于完成了，太激动了还有一个评论功能没有添加，好像有点麻烦，过段时间再来搞吧]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>杂谈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[王子]]></title>
    <url>%2F2019%2F03%2F17%2F02_%E7%8E%8B%E5%AD%90%2F</url>
    <content type="text"><![CDATA[我的博客终于搭建好了，呜呜呜，哭唧唧搭，忍不住要发第一篇博文 在这春天，总是有人让我记起高中时我们班的那位“王子”（真名：王子X）。他恰如我笔下所描绘的那样……他的身上总是能散发出一种味道，那味道是。。。他走路总是有一种气质，摇摇欲坠，翩翩欲倒他擦鼻涕有一种独特的方式，袖子一提，手横起一揩，没了他总是有吃不完的辣条[羡慕]，偶尔嘴上还留有油渍他能修电脑，总是受到语文老师独一无二的青睐，比如，某天，偷偷地（不，不知他有什么独特的方法）溜出校门去买午餐还是晚餐来着，坐在某幢楼的角落秀色可餐的吃起来，然后…就被我们语文老师逮个正着语文老师用自己婀娜的身姿，迷人的手势，忍俊不禁的笑容向我们讲及这件事高一还是高二来着，我们十分有幸分到了一个小组，当时班主任叫我们给自己组想一个口号，别的小组都是什么“赶莺超美”（传说中我们班的学霸），而我们组的却是：清新口气，你我更亲近然而…如今…历史总是惊人的相似…我…have no words我犹记得，高中学校外有一家饰品店，外面拉了一条横幅，大意是“一直被模仿，从未被超越”yes，目前，他奶奶的，我深刻地把前半句话理解得透彻透底most important！THIS IS NOT THE FIRST TIME!!!莫非？我的idea太好了？我常常因为自己过于优秀而感到苦恼 我抄你个奶奶的腿儿的 最后！是王子走进了这家店吗？？？]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>学校</tag>
        <tag>杂谈</tag>
      </tags>
  </entry>
</search>
