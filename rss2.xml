<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Awake&#39;s Blog</title>
    <link>http://yoursite.com/</link>
    <atom:link href="/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description></description>
    <pubDate>Thu, 02 May 2019 12:19:10 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>五月伊始</title>
      <link>http://yoursite.com/2019/05/02/%E4%BA%94%E6%9C%88%E4%BC%8A%E5%A7%8B/</link>
      <guid>http://yoursite.com/2019/05/02/%E4%BA%94%E6%9C%88%E4%BC%8A%E5%A7%8B/</guid>
      <pubDate>Thu, 02 May 2019 13:21:30 GMT</pubDate>
      <description>
      
        &lt;p&gt;我以为世界在等着我长大，没想到是时间推着我向前&lt;br&gt;&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>我以为世界在等着我长大，没想到是时间推着我向前<br><a id="more"></a><br>感觉时间飞逝啊！上一次写博客已经是很久前的事了吧！四月已经结束了！五月又匆匆忙忙地来了！我还是没有什么进步。<br>这个月计算机二级的成绩也要出来了，英语教师资格证面试也快了，最重要的是，七月就要去实习了，五月就要报名是集中实习还是分散实习了，所以我，得开始准备面试了。不管是爬虫还是新闻，简历都备起来了。在写简历的时候才发现自己什么也写不上去。<br>可是生活还是要继续啊，我在努力啊。我也想把年薪过万变成现实啊，想去旅游，想带奶奶吃好吃的，想带弟弟玩好玩的。<br>最近，我是被恋爱冲昏了头脑吧，不！我还没恋呢，不过为什么我那么依赖他，居然在他面前变成了一个小女生？？？what fuck？？？我怎么会变成这种人？？？我不能这样啊，我要好好学习啊，不能浪费时间了。<br>今天他说带我回家过年，虽然我知道可能是玩笑话，不过我还是认真的告诉了他我不愿意。<br>我不是觉得自己要和很优秀（高富帅）的人在一起，毕竟我自己也不怎样，好男人又没瞎眼，毕竟物以类聚，人以群分，可是我还是不想结婚，不是和谁的问题，是我不想。我想象不出来自己结婚后是怎样的？我都怀疑自己要单身一辈子了！<br>也遇不到一个合适的，我觉得就应该听妈妈的话，好好地去相亲，毕竟妈妈看上的人家都不是很差。</p><p>不不不！谈什么儿女情长，在这个年龄就应该巨富！！！好好努力吧，老老实实写代码，勤勤恳恳找实习！</p><p>（づ￣3￣）づ╭❤～</p>]]></content:encoded>
      
      <comments>http://yoursite.com/2019/05/02/%E4%BA%94%E6%9C%88%E4%BC%8A%E5%A7%8B/#disqus_thread</comments>
    </item>
    
    <item>
      <title>网易云单曲评论</title>
      <link>http://yoursite.com/2019/05/02/%E7%BD%91%E6%98%93%E4%BA%91%E5%8D%95%E6%9B%B2%E8%AF%84%E8%AE%BA/</link>
      <guid>http://yoursite.com/2019/05/02/%E7%BD%91%E6%98%93%E4%BA%91%E5%8D%95%E6%9B%B2%E8%AF%84%E8%AE%BA/</guid>
      <pubDate>Thu, 02 May 2019 11:05:14 GMT</pubDate>
      <description>
      
        &lt;p&gt;眼看着7月份就要去实习了，还不知道做什么呢，什么都还不会呢，打算着找个爬虫的实习吧，但是我这点水平应该不行吧，所以又来写了个网易单曲评论爬虫。&lt;br&gt;感觉好！不能这么丧！振作！快醒醒！起来撸代码啦！&lt;br&gt;&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>眼看着7月份就要去实习了，还不知道做什么呢，什么都还不会呢，打算着找个爬虫的实习吧，但是我这点水平应该不行吧，所以又来写了个网易单曲评论爬虫。<br>感觉好！不能这么丧！振作！快醒醒！起来撸代码啦！<br><a id="more"></a><br>看了一下网易云，以陈雪凝的《绿色》为例，F12键找到了这个R_SO_4_1345848098?csrf_token=，并且点开发现了里面就是我们想要的评论数据<br><img src="/2019/05/02/网易云单曲评论/1.png"><br>再查看请求头,这个requet url：<br><a href="https://music.163.com/weapi/v1/resource/comments/R_SO_4_1345848098?csrf_token=" target="_blank" rel="noopener">https://music.163.com/weapi/v1/resource/comments/R_SO_4_1345848098?csrf_token=</a><br>看request method是以post方式提交的<br><img src="/2019/05/02/网易云单曲评论/2.png"><br>然后我屁颠儿屁颠儿地点了个翻页，在评论翻页的时候这个request url并没有刷新！！！<br>没有刷新那这个翻页的操作怎么做？<br>是用selenium还是…？<br>看下面的表单数据应该是经过加密的，所以我在网上百度了一下，果然有大佬破解这个算法加密，参考：<a href="https://www.zhihu.com/question/36081767" target="_blank" rel="noopener">https://www.zhihu.com/question/36081767</a><br><img src="/2019/05/02/网易云单曲评论/3.png"><img src="/2019/05/02/网易云单曲评论/4.png"><br>这是算法解密的代码，都是大佬们牛逼啊！<br>我就稍微改了一下，并且把数据存储到MongoDB</p><p>完整代码地址见：<a href="https://github.com/Awake2714/WangyiComment" target="_blank" rel="noopener">https://github.com/Awake2714/WangyiComment</a></p><p>不过有点问题的是，我发现在爬到400多页的时候就没数据了，虽然代码一直在跑，而且也没有被ban，但是就是没拿到数据，所以这是怎么回事？是不是像豆瓣一样，评论只能拿取500条？有没有小伙伴知道呀？</p>]]></content:encoded>
      
      <comments>http://yoursite.com/2019/05/02/%E7%BD%91%E6%98%93%E4%BA%91%E5%8D%95%E6%9B%B2%E8%AF%84%E8%AE%BA/#disqus_thread</comments>
    </item>
    
    <item>
      <title>利用scrapy框架下载煎蛋网图片</title>
      <link>http://yoursite.com/2019/04/18/jandan/</link>
      <guid>http://yoursite.com/2019/04/18/jandan/</guid>
      <pubDate>Thu, 18 Apr 2019 13:21:30 GMT</pubDate>
      <description>
      
        &lt;p&gt;每次写完代码后都感觉自己人老珠黄啊，该拿什么拯救我的老脸？？&lt;br&gt;&lt;img src=&quot;/2019/04/18/jandan/7.png&quot;&gt;&lt;br&gt;&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>每次写完代码后都感觉自己人老珠黄啊，该拿什么拯救我的老脸？？<br><img src="/2019/04/18/jandan/7.png"><br><a id="more"></a></p><h2 id="spider-py"><a href="#spider-py" class="headerlink" title="spider.py"></a>spider.py</h2><p>这部分很简单，只需要获取图片链接和下一页的链接就可以了<br><img src="/2019/04/18/jandan/4.png"></p><h2 id="item-py"><a href="#item-py" class="headerlink" title="item.py"></a>item.py</h2><blockquote><p>import scrapy<br>class JiandanItem(scrapy.Item):<br>&ensp;&ensp;&ensp;&ensp;img = scrapy.Field()</p></blockquote><h2 id="pipelines-py"><a href="#pipelines-py" class="headerlink" title="pipelines.py"></a>pipelines.py</h2><p>这一部分比较重要，scrapy提供了ImagesPipeline，我们只需要继承自这个类，重写其中的一些方法，scrapy就会为我们下载图片了。<br><img src="/2019/04/18/jandan/6.png"></p><h2 id="settings-py"><a href="#settings-py" class="headerlink" title="settings.py"></a>settings.py</h2><p>在settings.py里开启相关配置<br><img src="/2019/04/18/jandan/3.png"><br><img src="/2019/04/18/jandan/2.png"></p><h2 id="部分图片展示"><a href="#部分图片展示" class="headerlink" title="部分图片展示"></a>部分图片展示</h2><p>因为煎蛋网反爬措施没那么厉害，所以很快“唰唰唰”的就下载完了<br>一共四千多张图片，慢慢欣赏吧    biu~ (๑´ڡ`๑)<br><img src="/2019/04/18/jandan/1.jpg"></p>]]></content:encoded>
      
      <comments>http://yoursite.com/2019/04/18/jandan/#disqus_thread</comments>
    </item>
    
    <item>
      <title>爬虫设置随机请求头和代理</title>
      <link>http://yoursite.com/2019/04/11/%E7%88%AC%E8%99%AB%E8%AE%BE%E7%BD%AE%E9%9A%8F%E6%9C%BA%E8%AF%B7%E6%B1%82%E5%A4%B4%E5%92%8C%E4%BB%A3%E7%90%86/</link>
      <guid>http://yoursite.com/2019/04/11/%E7%88%AC%E8%99%AB%E8%AE%BE%E7%BD%AE%E9%9A%8F%E6%9C%BA%E8%AF%B7%E6%B1%82%E5%A4%B4%E5%92%8C%E4%BB%A3%E7%90%86/</guid>
      <pubDate>Thu, 11 Apr 2019 13:55:39 GMT</pubDate>
      <description>
      
        &lt;p&gt;我在设置爬虫代理和请求头时，真是遇到了很多坑啊，自己一个人自学，又没人交流，自己又是一个没有耐心的人，简直要爆炸了&lt;br&gt;&lt;img src=&quot;/2019/04/11/爬虫设置随机请求头和代理/img2.jpg&quot; align=&quot;center&quot;&gt;&lt;br&gt;终于，今天晚上好好地静下来慢慢捋了一下代码！&lt;br&gt;终于，似乎从这个坑出来了！&lt;br&gt;&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>我在设置爬虫代理和请求头时，真是遇到了很多坑啊，自己一个人自学，又没人交流，自己又是一个没有耐心的人，简直要爆炸了<br><img src="/2019/04/11/爬虫设置随机请求头和代理/img2.jpg" align="center"><br>终于，今天晚上好好地静下来慢慢捋了一下代码！<br>终于，似乎从这个坑出来了！<br><a id="more"></a><br>先来说说用requests库设置吧，随机请求头我用的是fake-useragent这个包<br>先用<span href="http://www.httpbin.org/get">httpbin.org/get</span>这个网址测试了一下<br><img src="/2019/04/11/爬虫设置随机请求头和代理/img3.png" align="center"><br>就这样设置就可以正确输出了，<br>开始我设置的时候居然直接写的 headers=ua.random 我真的要哭死啊！！！<br>一定要记得构造字典！！！</p><p>设置代理，我买的蘑菇代理，感觉还挺划算的，按数量买1000个6块钱，毕竟测试也用不完这么多，永久有效，爬完某宝后我都还剩700多个；按数量感觉不太划算，一天2000个6块钱吧，不过我也用不完，当然如果你需求大的话另当别论，土豪也请随意哈哈哈<br><img src="/2019/04/11/爬虫设置随机请求头和代理/img4.png" align="center"><br>这里的坑也很多啊，请求了好多次后都不成功，我把ip打印了一下，好像确实有点问题，不过还是不知道问题确切所在，所以又打印了一下长度，果然！！！里面有个换行符和回车！！！我的娘啊！！！我枯了<br>皇天不负有心人，终于搞好了<br><img src="/2019/04/11/爬虫设置随机请求头和代理/img5.jpg" align="center"></p><p>接下来终于在scrapy框架里面设置这两个中间件了<br><img src="/2019/04/11/爬虫设置随机请求头和代理/img6.png" align="center"><br>在网上看到好多方法，自己也懵逼，试了一下，这样也行的：<br>request.headers[‘User-Agent’] = self.ua.random<br>不过我真是傻！这样好像是真的不行！不行！会报错的！<br>request.meta[‘headers’] = {‘User-Agent’: ua.random}<br>因为有些请求是不需要代理的，所以查了一下说什么重写make_requests_from_url()方法，设置meta={‘download_timeout’: 10}，但是不知道为什么我的就是不行，所以干脆没有设置了，如果有大佬知道是怎么回事可不可以告诉我一下啊。</p><p>另外我是想运行这个代理池的，但是老是报错，实在不知怎么回事啊[泪奔]<br><img src="/2019/04/11/爬虫设置随机请求头和代理/img7.png" align="center"></p>]]></content:encoded>
      
      <comments>http://yoursite.com/2019/04/11/%E7%88%AC%E8%99%AB%E8%AE%BE%E7%BD%AE%E9%9A%8F%E6%9C%BA%E8%AF%B7%E6%B1%82%E5%A4%B4%E5%92%8C%E4%BB%A3%E7%90%86/#disqus_thread</comments>
    </item>
    
    <item>
      <title>scrapy爬取某宝商品信息</title>
      <link>http://yoursite.com/2019/04/11/scrapy%E7%88%AC%E5%8F%96%E6%9F%90%E5%AE%9D%E5%95%86%E5%93%81%E4%BF%A1%E6%81%AF/</link>
      <guid>http://yoursite.com/2019/04/11/scrapy%E7%88%AC%E5%8F%96%E6%9F%90%E5%AE%9D%E5%95%86%E5%93%81%E4%BF%A1%E6%81%AF/</guid>
      <pubDate>Thu, 11 Apr 2019 13:00:52 GMT</pubDate>
      <description>
      
        &lt;p&gt;在多次踩坑后，终于写好我的爬虫了。&lt;br&gt;说说我遇到的问题:&lt;br&gt;第一就是在scrapy框架里设置随机请求头中间件&lt;br&gt;第二也是中间件，代理中间件，关于这两点可以参考我的博客：&lt;br&gt;&lt;a href=&quot;http://www.shuxuelian.top/2019/04/11/%E7%88%AC%E8%99%AB%E8%AE%BE%E7%BD%AE%E9%9A%8F%E6%9C%BA%E8%AF%B7%E6%B1%82%E5%A4%B4%E5%92%8C%E4%BB%A3%E7%90%86/#more&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;爬虫设置随机请求头和代理&lt;/a&gt;&lt;br&gt;这两个问题真的困扰了我超级久啊。&lt;br&gt;解决好这两个问题好，我的爬虫终于运行起来啦啦啦&lt;br&gt;&lt;img src=&quot;/2019/04/11/scrapy爬取某宝商品信息/img1.jpg&quot; align=&quot;center&quot; style=&quot;width: 400px; height: 400px&quot;&gt;&lt;br&gt;&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>在多次踩坑后，终于写好我的爬虫了。<br>说说我遇到的问题:<br>第一就是在scrapy框架里设置随机请求头中间件<br>第二也是中间件，代理中间件，关于这两点可以参考我的博客：<br><a href="http://www.shuxuelian.top/2019/04/11/%E7%88%AC%E8%99%AB%E8%AE%BE%E7%BD%AE%E9%9A%8F%E6%9C%BA%E8%AF%B7%E6%B1%82%E5%A4%B4%E5%92%8C%E4%BB%A3%E7%90%86/#more" target="_blank" rel="noopener">爬虫设置随机请求头和代理</a><br>这两个问题真的困扰了我超级久啊。<br>解决好这两个问题好，我的爬虫终于运行起来啦啦啦<br><img src="/2019/04/11/scrapy爬取某宝商品信息/img1.jpg" align="center" style="width: 400px; height: 400px"><br><a id="more"></a><br>贴一下我的部分代码，嘿嘿嘿</p><p><strong style="font-size: 24px; color: #000000">目录</strong><br>文件夹结构<br><img src="/2019/04/11/scrapy爬取某宝商品信息/img9.png" align="center"></p><p><strong style="font-size: 24px; color: #000000">taobao.py</strong><br>重写 start_requests() 方法<br>parse()方法 解析商品信息<br><img src="/2019/04/11/scrapy爬取某宝商品信息/img3.png" align="center"></p><p><strong style="font-size: 24px; color: #000000">items.py</strong><br>你要存储的字段<br><img src="/2019/04/11/scrapy爬取某宝商品信息/img8.png" align="center"></p><p><strong style="font-size: 24px; color: #000000">middlewares.py</strong><br>随机请求头 UserAgentMiddleware<br>代理 ProxyMiddleware<br><img src="/2019/04/11/scrapy爬取某宝商品信息/img4.png" align="center"></p><p><strong style="font-size: 24px; color: #000000">settings.py</strong><br><img src="/2019/04/11/scrapy爬取某宝商品信息/img6.png" align="center"><br><img src="/2019/04/11/scrapy爬取某宝商品信息/img7.png"></p><p><strong style="font-size: 24px; color: #000000">pipelines.py</strong><br>存储数据到Mongodb<br><img src="/2019/04/11/scrapy爬取某宝商品信息/img5.png" align="center"></p><p><strong style="font-size: 24px; color: #000000">run.py</strong><br>最后就可以运行啦 python run.py<br><img src="/2019/04/11/scrapy爬取某宝商品信息/img10.png" align="center"><br>运行结束后的部分数据是这样的哈哈哈<br><img src="/2019/04/11/scrapy爬取某宝商品信息/img11.png"></p><p>代码地址：<a href="https://github.com/Awake2714/Taobao" target="_blank" rel="noopener">https://github.com/Awake2714/Taobao</a></p>]]></content:encoded>
      
      <comments>http://yoursite.com/2019/04/11/scrapy%E7%88%AC%E5%8F%96%E6%9F%90%E5%AE%9D%E5%95%86%E5%93%81%E4%BF%A1%E6%81%AF/#disqus_thread</comments>
    </item>
    
  </channel>
</rss>
